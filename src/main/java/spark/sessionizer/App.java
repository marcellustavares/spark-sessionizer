/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package spark.sessionizer;

import java.util.Arrays;
import java.util.concurrent.TimeUnit;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoder;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.KeyValueGroupedDataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.GroupState;
import org.apache.spark.sql.streaming.GroupStateTimeout;
import org.apache.spark.sql.streaming.OutputMode;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.streaming.Trigger;
import org.apache.spark.sql.types.ArrayType;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructType;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.StreamingContext;
import scala.Function1;
import scala.Function3;
import scala.Option;
import scala.Tuple3;
import scala.collection.Iterator;

public class App {
    public static void main(String[] args) throws StreamingQueryException {
        SparkSession sparkSession = SparkSession.builder()
            .appName("Sessionization: streaming approach").master("spark://localhost:7077")
            .config("spark.streaming.stopGracefullyOnShutdown", "true")
            .getOrCreate();

        sparkSession.sparkContext().setLogLevel("ERROR");

        Dataset<Row> dataset = sparkSession.readStream()
            .format("socket")
            .option("host", "localhost")
            .option("port", "9999")
            .option("spark.streaming.stopGracefullyOnShutdown", "true")
            .load();

        long sessionTimeout = TimeUnit.MINUTES.toMillis(1);

        StructType schema = DataTypes.createStructType(
            Arrays.asList(
                DataTypes.createStructField("userId", DataTypes.StringType, false),
                DataTypes.createStructField("eventDate", DataTypes.TimestampType, false),
                DataTypes.createStructField("eventId", DataTypes.StringType, false))
        );

       // Function3<String, Iterator<Row>, GroupState<UserSession>, UserSession> eventsToSessions = new Mapping.EventsToSessions();

//        new Thread(() -> {
//            try {
//                System.out.println("Wait 10s");
//                Thread.sleep(10_000);
//
//            } catch (InterruptedException e) {
//                e.printStackTrace();
//            }
//            System.out.println("stopping spark ");
//            new StreamingContext(sparkSession.sparkContext(), Duration.apply(1000)).stop(true, true);
//
//        }).start();

        StreamingQuery streamingQuery = dataset.selectExpr("CAST(value AS STRING)")
            .select(functions.from_json(functions.col("value"), schema).as("data"))
            .select("data.*")
            .withWatermark("eventDate", "1 minutes")
            .groupByKey((MapFunction<Row, String>) row -> row.getAs("userId"), Encoders.STRING())
            .mapGroupsWithState(GroupStateTimeout.ProcessingTimeTimeout(), new Mapping.EventsToSessions(), Encoders.bean(UserSession.class), Encoders.STRING())
            //.flatMapGroupsWithState(OutputMode.Append(),GroupStateTimeout.EventTimeTimeout(), new Mapping.EventsToEvents(), Encoders.bean(UserSession.class), Encoders.STRING())

            .writeStream()
            .outputMode(OutputMode.Update())
            .option("checkpointLocation", "/tmp/osbasah-sessionization-checkpoint")
            .format("console")
            .option("truncate", "false")
            //.outputMode("complete")
            .start();

//        new Thread(() -> {
//            try {
//                System.out.println("Wait 10s");
//                Thread.sleep(10_000);
//
//            } catch (InterruptedException e) {
//                e.printStackTrace();
//            }
//            System.out.println("stopping spark ");
//            //new StreamingContext(sparkSession.sparkContext(), Duration.apply(1000)).stop(true, true);
//            streamingQuery.stop();
//
//        }).start();

        streamingQuery.awaitTermination();

    }



}
